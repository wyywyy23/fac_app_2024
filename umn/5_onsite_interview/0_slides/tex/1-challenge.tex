%!TEX root = ../research_overview.tex

\section{Grand Challenge}

\subsection{Communication Bottleneck}

\begin{frame}{Challenges Moving Data Off-Chip}
    \only<2-2|handout:0>{\centering\includegraphics[width=\textwidth]{fig/bw_taper-1_compressed.pdf}}%
    \only<3-3|handout:0>{\centering\includegraphics[width=\textwidth]{fig/bw_taper-2_compressed.pdf}}%
    \only<4-4|handout:0>{\centering\includegraphics[width=\textwidth]{fig/bw_taper-3_compressed.pdf}}%
    \only<5-5|handout:0>{\centering\includegraphics[width=\textwidth]{fig/bw_taper-4_compressed.pdf}}%
    \only<6-6>{\centering\includegraphics[width=\textwidth]{fig/bw_taper-5_compressed.pdf}}%
    \note<1-1>[item]{Let's first look at a big challenge in today's large-scale computing systems, which is the communication bottleneck.}
    \note<2-2>[item]{More specifically, in today's systems, data from one computing node needs to travel across several levels of network hierarchy to reach another computing node.}
    \note<3-3>[item]{When this data is moved within the socket, like between the GPU and the memory, we see that the bandwidth there is actually tremendous, up to several terabytes per second. And the energy efficiency is really great, in the order of femtojoules per bit. This bandwidth and energy efficiency are achieved by using very dense and parallel electrical interconnects, like a highway for digital data with many many lanes.}
    \note<4-4>[item]{As we move to the board level, and look between the GPUs, we see that the bandwidth drops by a factor of 5 or so, because the signals need to travel a bit longer in distance and perhaps through a switch fabric. But it is still quite good, well above 1 terabyte per second in those best-in-class commercial solutions. And the energy consumption is about tens to hundreds of femtojoules per bit.}
    \note<5-5>[item]{What really becomes dramatic is when the data needs to go off-board and travel several meters or even hundreds of meters across the data center. This is where the bandwidth plunges to below 1 tera BIT per second, not byte. This is also where traditionally optical signaling start to get used, in the form of pluggable optical transceivers, but they are nowhere near energy efficient, usually consuming more than 20 picojoules per bit.}
    \note<6-6>[item]{So it's really like having a superhighway of data that suddenly narrows down to a dirt road. And because of this, we are seeing a two order of magnitude, maybe even more, bandwidth discrepancy across the network hierarchy, that limits the deployment of large-scale computational tasks onto more computing nodes.}
    \note<6-6>[item]{And let me show you why we have to address this problem immediately.}

\end{frame}

\subsection{Why Now?}

\begin{frame}{AI Applications Driving Explosive Growth}

    \only<2-2|handout:0>{\centering\includegraphics[width=0.6\textwidth]{fig/model_growth-1_compressed.pdf}}%
    \only<3-3|handout:0>{\centering\includegraphics[width=0.6\textwidth]{fig/model_growth-2_compressed.pdf}}%
    \only<4-4>{\centering\includegraphics[width=0.6\textwidth]{fig/model_growth-3_compressed.pdf}}%
    \vspace{-1em}%
    \begin{center}%
        \onslide<2->{\fullcite{wuPetaScaleEmbeddedPhotonics2023}}%
    \end{center}%
    \note<1-1>[item]{We all know that Moore's Law has been slowing down and coming to an end.}
    \note<2-2>[item]{But the growth of computational demand is not slowing down. This is especially driven by the explosive growth of AI applications.}
    \note<2-2>[item]{As we can see here, the size of AI models has been growing nearly an order of magnitude per year over the past six to eight years, and the largest model has already reached 100 trillion parameters.}
    \note<3-3>[item]{And because there is no way to fit these models into any single compute unit, there has been tremendous effort from software engineering to program around data locality, knowing that long-distance data movement is very expensive.}
    \note<4-4>[item]{But as these models grow larger and larger, long-distance communication is becoming unavoidable in the computation of these workloads.}
    \note<4-4>[item]{So, if the bandwidth and energy efficiency issues of these long-distance communications are not addressed, here is what will happen.}

\end{frame}

\begin{frame}{Per-Training Energy Consumption}

    \only<1-1|handout:0>{\centering\includegraphics[width=0.8\textwidth]{fig/energy_growth-1_compressed.pdf}}%
    \only<2-2|handout:0>{\centering\includegraphics[width=0.8\textwidth]{fig/energy_growth-2_compressed.pdf}}%
    \only<3-3|handout:0>{\centering\includegraphics[width=0.8\textwidth]{fig/energy_growth-3_compressed.pdf}}%
    \only<4-4>{\centering\includegraphics[width=0.8\textwidth]{fig/energy_growth-4_compressed.pdf}}%
    \vspace{-1em}%
    \begin{center}%
        \fullcite{liangHolisticEvaluationLanguage2023,pattersonCarbonEmissionsLarge2021,masanetRecalibratingGlobalData2020}%
    \end{center}%
    \note<1-1>[item]{We see the tremendous growth in the energy consumption of these workloads, especially over the time it takes to complete the training of these models over a large distributed computing cluster.}
    \note<2-2>[item]{For example, GPT-4, which is marked on the top right corner, is estimated to be trained on 25 thousand GPUs, costing over 51 thousand megawatt-hours of energy, and it takes over 3 months to finish the training.}
    \note<3-3>[item]{And just to put it in some perspective, this megawatt-hour number for training a single AI model is greater than the hourly electricity used by the entire New York City in a hot summer day.}
    \note<4-4>[item]{So, this energy cost has really become environmentally significant, and is preventing the system from being able to further scale and support a wide range of real-world applications, like climate prediction, drug discovery, financial modeling, and defense.}
    \note<4-4>[item]{Now, how can we use integrated photonics to address this bandwidth and energy problem?}


\end{frame}