@inproceedings{keelerERI2019,
  author    = {Keeler, Gordon},
  booktitle = {DARPA ERI Summit},
  year      = {2019}
}

@misc{liangHolisticEvaluationLanguage2023,
  abstract      = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  archiveprefix = {arXiv},
  author        = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R{\'e}, Christopher and {Acosta-Navas}, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  doi           = {10.48550/arXiv.2211.09110},
  eprint        = {2211.09110},
  file          = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Liang et al_2023_Holistic Evaluation of Language Models.pdf;/Users/wyy/Zotero/storage/QKGK2C3K/2211.html},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  month         = oct,
  number        = {arXiv:2211.09110},
  publisher     = {arXiv},
  title         = {Holistic {{Evaluation}} of {{Language Models}}},
  urldate       = {2024-04-05},
  year          = {2023}
}

@article{masanetRecalibratingGlobalData2020,
  author    = {Masanet, Eric and Shehabi, Arman and Lei, Nuoa and Smith, Sarah and Koomey, Jonathan},
  doi       = {10.1126/science.aba3758},
  file      = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Masanet et al_2020_Recalibrating global data center energy-use estimates.pdf},
  journal   = {Science},
  month     = feb,
  number    = {6481},
  pages     = {984--986},
  publisher = {American Association for the Advancement of Science},
  title     = {Recalibrating Global Data Center Energy-Use Estimates},
  urldate   = {2024-04-05},
  volume    = {367},
  year      = {2020}
}

@misc{pattersonCarbonEmissionsLarge2021,
  abstract      = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  author        = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  doi           = {10.48550/arXiv.2104.10350},
  eprint        = {2104.10350},
  file          = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Patterson et al_2021_Carbon Emissions and Large Neural Network Training.pdf;/Users/wyy/Zotero/storage/7BSHF4WG/2104.html},
  keywords      = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  month         = apr,
  number        = {arXiv:2104.10350},
  publisher     = {arXiv},
  title         = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  urldate       = {2024-04-05},
  year          = {2021}
}

@article{wangCoDesignedSilicon2024,
  author  = {Wang, Yuyang and Wang, Songli and Parsons, Robert and Sanyal, Swarnava and Gopal, Vignesh and Novick, Asher and Rizzo, Anthony and Lipson, Michal and Gaeta, Alexander L. and Bergman, Keren},
  doi     = {10.1109/TCPMT.2024.3492189},
  journal = {IEEE T-CPMT},
  title   = {Co-Designed Silicon Photonics Chip {I/O} for Energy-Efficient Petascale Connectivity},
  year    = {2024}
}

@article{wuPetaScaleEmbeddedPhotonics2023,
  author   = {Wu, Zhenguo and Dai, Liang Yuan and Novick, Asher and Glick, Madeleine and Zhu, Ziyi and Rumley, S{\'e}bastien and Michelogiannakis, George and Shalf, John and Bergman, Keren},
  doi      = {10.1109/JLT.2023.3276588},
  file     = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Wu et al_2023_Peta-Scale Embedded Photonics Architecture for Distributed Deep Learning2.pdf;/Users/wyy/Zotero/storage/V96R3SFX/10124987.html},
  issn     = {0733-8724, 1558-2213},
  journal  = {J. Light. Technol.},
  keywords = {Bandwidth,collective communication,Computational modeling,Copper,Distributed deep learning,optical interconnect,Optical switches,silicon photonics,Silicon photonics,Topology,Training},
  number   = {12},
  pages    = {3737--3749},
  title    = {Peta-{{Scale Embedded Photonics Architecture}} for {{Distributed Deep Learning Applications}}},
  urldate  = {2024-03-18},
  volume   = {41},
  year     = {2023}
}
