@article{jamesScalingCombdrivenResonatorbased2023,
  author  = {James, Aneek and Novick, Asher and Rizzo, Anthony and Parsons, Robert and Jang, Kaylx and Hattink, Maarten and Bergman, Keren},
  doi     = {10.1364/OPTICA.491756},
  journal = {Optica},
  month   = jul,
  number  = {7},
  pages   = {832--840},
  title   = {Scaling Comb-Driven Resonator-Based {{DWDM}} Silicon Photonic Links to Multi-{{Tb}}/s in the Multi-{{FSR}} Regime},
  urldate = {2023-11-02},
  volume  = {10},
  year    = {2023}
}

@inproceedings{keelerERI2019,
  author    = {Keeler, Gordon},
  booktitle = {DARPA ERI Summit},
  year      = {2019}
}

@article{kimTurnkeyHighefficiencyKerr2019,
  author  = {Kim, Bok Young and Okawachi, Yoshitomo and Jang, Jae K. and Yu, Mengjie and Ji, Xingchen and Zhao, Yun and Joshi, Chaitanya and Lipson, Michal and Gaeta, Alexander L.},
  doi     = {10.1364/OL.44.004475},
  file    = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Kim et al_2019_Turn-key, high-efficiency Kerr comb source.pdf;/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Kim et al_2019_Turn-key, high-efficiency Kerr comb source2.pdf;/Users/wyy/Zotero/storage/SPYPNGPV/fulltext.html},
  issn    = {0146-9592, 1539-4794},
  journal = {Optics Letters},
  langid  = {english},
  month   = sep,
  number  = {18},
  pages   = {4475},
  title   = {Turn-Key, High-Efficiency {{Kerr}} Comb Source},
  urldate = {2022-12-31},
  volume  = {44},
  year    = {2019}
}

@misc{liangHolisticEvaluationLanguage2023,
  abstract      = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  archiveprefix = {arXiv},
  author        = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R{\'e}, Christopher and {Acosta-Navas}, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  doi           = {10.48550/arXiv.2211.09110},
  eprint        = {2211.09110},
  file          = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Liang et al_2023_Holistic Evaluation of Language Models.pdf;/Users/wyy/Zotero/storage/QKGK2C3K/2211.html},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  month         = oct,
  number        = {arXiv:2211.09110},
  publisher     = {arXiv},
  title         = {Holistic {{Evaluation}} of {{Language Models}}},
  urldate       = {2024-04-05},
  year          = {2023}
}

@article{masanetRecalibratingGlobalData2020,
  author    = {Masanet, Eric and Shehabi, Arman and Lei, Nuoa and Smith, Sarah and Koomey, Jonathan},
  doi       = {10.1126/science.aba3758},
  file      = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Masanet et al_2020_Recalibrating global data center energy-use estimates.pdf},
  journal   = {Science},
  month     = feb,
  number    = {6481},
  pages     = {984--986},
  publisher = {American Association for the Advancement of Science},
  title     = {Recalibrating Global Data Center Energy-Use Estimates},
  urldate   = {2024-04-05},
  volume    = {367},
  year      = {2020}
}

@misc{pattersonCarbonEmissionsLarge2021,
  abstract      = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  author        = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  doi           = {10.48550/arXiv.2104.10350},
  eprint        = {2104.10350},
  file          = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Patterson et al_2021_Carbon Emissions and Large Neural Network Training.pdf;/Users/wyy/Zotero/storage/7BSHF4WG/2104.html},
  keywords      = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  month         = apr,
  number        = {arXiv:2104.10350},
  publisher     = {arXiv},
  title         = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  urldate       = {2024-04-05},
  year          = {2021}
}

@article{rizzoPetabitScaleSiliconPhotonic2023,
  abstract = {Silicon photonics holds significant promise in revolutionizing optical interconnects in data centers and high performance computers to enable scaling into the Pb/s package escape bandwidth regime while consuming orders of magnitude less energy per bit than current solutions. In this work, we review recent progress in silicon photonic interconnects leveraging chip-scale Kerr frequency comb sources and provide a comprehensive overview of massively scalable silicon photonic systems capable of capitalizing on the large number of wavelengths provided by such combs. We first consider the high-level architectural constraints and then proceed to detail the corresponding fundamental device designs supported by both simulated and experimental results. Furthermore, the majority of experimentally measured devices were fabricated in a commercial 300 mm foundry, showing a clear path to volume manufacturing. Finally, we present various system-level experiments which illustrate successful proof-of-principle operation, including flip-chip integration with a co-designed CMOS application-specific integrated circuit (ASIC) to realize a complete Kerr comb-driven electronic-photonic engine. These results provide a viable and appealing path towards future co-packaged silicon photonic interconnects with aggregate per-fiber bandwidth above 1 Tb/s, energy consumption below 1 pJ/bit, and areal bandwidth density greater than 5 Tb/s/mm2.},
  author   = {Rizzo, Anthony and Daudlin, Stuart and Novick, Asher and James, Aneek and Gopal, Vignesh and Murthy, Vaishnavi and Cheng, Qixiang and Kim, Bok Young and Ji, Xingchen and Okawachi, Yoshitomo and {van Niekerk}, Matthew and Deenadayalan, Venkatesh and Leake, Gerald and Fanto, Michael and Preble, Stefan and Lipson, Michal and Gaeta, Alexander and Bergman, Keren},
  doi      = {10.1109/JSTQE.2022.3197375},
  issn     = {1558-4542},
  journal  = {IEEE J. Sel. Top. Quant. Electron.},
  keywords = {Bandwidth,Channel spacing,Computer architecture,Integrated circuit interconnections,Optical filters,optical frequency combs,Optical resonators,Silicon photonics,wavelength division multiplexing},
  month    = jan,
  number   = {1: Nonlinear Integrated Photonics},
  pages    = {1--20},
  title    = {Petabit-{{Scale Silicon Photonic Interconnects With Integrated Kerr Frequency Combs}}},
  volume   = {29},
  year     = {2023}
}

@article{wangAutomatedTuningRingassisted2025,
  abstract = {We demonstrate a compact ring-assisted Mach--Zehnder interferometer (RAMZI)-based silicon photonic interleaver with a 400\,GHz free spectral range (FSR), featuring flat passbands exceeding a spectral range of 50\,nm. Additionally, we introduce a novel, to the best of our knowledge, add-on structure and tuning method enabling automated compensation for fabrication imperfections, precise shaping of the RAMZI flat-top passbands, and alignment with Kerr comb lines. Experimental results have shown successful interleaving of eight channels of distributed-feedback (DFB) lasers as well as a 200\,GHz Kerr comb, both achieving an extinction ratio of approximately 20\,dB.},
  author   = {Wang, Songli and Wang, Yuyang and Sanyal, Swarnava and Parsons, Robert and Ji, Xingchen and Okawachi, Yoshitomo and Meng, Xiang and Lipson, Michal and Gaeta, Alexander and Bergman, Keren},
  doi      = {10.1364/OL.546772},
  file     = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Wang et al. - 2025 - Automated tuning of a ring-assisted MZI-based interleaver for Kerr frequency combs.pdf},
  issn     = {0146-9592, 1539-4794},
  journal  = {Optics Letters},
  langid   = {english},
  month    = jan,
  number   = {2},
  pages    = {698},
  title    = {Automated Tuning of a Ring-Assisted {{MZI-based}} Interleaver for {{Kerr}} Frequency Combs},
  urldate  = {2025-01-20},
  volume   = {50},
  year     = {2025}
}


@inproceedings{wangCharacterizationApplicationsSpatial2020,
  author    = {Wang, Yuyang and Hulme, Jared and Sun, Peng and Jain, Mudit and Seyedi, M. Ashkan and Fiorentino, Marco and Beausoleil, Raymond G. and Cheng, Kwang-Ting},
  booktitle = {DAC},
  copyright = {All rights reserved},
  doi       = {10.1109/DAC18072.2020.9218608},
  file      = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Wang et al_2020_Characterization and Applications of Spatial Variation Models for Silicon.pdf},
  keywords  = {Erbium,Optical device fabrication,Optical resonators,Optical transmitters,Silicon,Transceivers},
  month     = jul,
  pages     = {1--6},
  title     = {Characterization and {{Applications}} of {{Spatial Variation Models}} for {{Silicon Microring-Based Optical Transceivers}}},
  urldate   = {2022-12-28},
  year      = {2020}
}

@inproceedings{wangScalableArchitectureSubpJ2023,
  author    = {Wang, Yuyang and Novick, Asher and Parsons, Robert and Wang, Songli and Jang, Kaylx and James, Aneek and Hattink, Maarten and Gopal, Vignesh and Rizzo, Anthony and Chiu, Chia-Pin and Hosseini, Kaveh and Hoang, Tim Tri and Bergman, Keren},
  booktitle = {SPIE Photonics West},
  doi       = {10.1117/12.2649506},
  month     = mar,
  pages     = {55},
  title     = {Scalable Architecture for Sub-{{pJ}}/b Multi-{{Tbps}} Comb-Driven {{DWDM}} Silicon Photonic Transceiver},
  urldate   = {2023-03-18},
  year      = {2023}
}

@article{wangEnergyEfficiencyYield2021,
  author    = {Wang, Yuyang and Sun, Peng and Hulme, Jared and Seyedi, M. Ashkan and Fiorentino, Marco and Beausoleil, Raymond G. and Cheng, Kwang-Ting},
  copyright = {All rights reserved},
  doi       = {10.1109/JLT.2020.3039489},
  file      = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Wang et al_2021_Energy Efficiency and Yield Optimization for Optical Interconnects via.pdf},
  issn      = {0733-8724, 1558-2213},
  journal   = {J. Light. Technol.},
  month     = mar,
  number    = {6},
  pages     = {1567--1578},
  title     = {Energy {{Efficiency}} and {{Yield Optimization}} for {{Optical Interconnects}} via {{Transceiver Grouping}}},
  urldate   = {2022-12-28},
  volume    = {39},
  year      = {2021}
}

@inproceedings{wangSiliconPhotonicsChip2024,
  author    = {Wang, Yuyang and Wang, Songli and Parsons, Robert and Novick, Asher and Gopal, Vignesh and Jang, Kaylx and Rizzo, Anthony and Chiu, Chia-Pin and Hosseini, Kaveh and Hoang, Tim Tri and Shumarayev, Sergey and Bergman, Keren},
  booktitle = {IEEE CICC},
  doi       = {10.1109/CICC60959.2024.10529018},
  month     = apr,
  pages     = {1--8},
  title     = {Silicon {{Photonics Chip I}}/{{O}} for {{Ultra High-Bandwidth}} and {{Energy-Efficient Die-to-Die Connectivity}}},
  urldate   = {2024-05-18},
  year      = {2024}
}

@article{wangCoDesignedSilicon2024,
  author  = {Wang, Yuyang and Wang, Songli and Parsons, Robert and Sanyal, Swarnava and Gopal, Vignesh and Novick, Asher and Rizzo, Anthony and Lipson, Michal and Gaeta, Alexander L. and Bergman, Keren},
  doi     = {10.1109/TCPMT.2024.3492189},
  journal = {IEEE T-CPMT},
  title   = {Co-Designed Silicon Photonics Chip {I/O} for Energy-Efficient Petascale Connectivity},
  year    = {2024}
}

@article{wuPetaScaleEmbeddedPhotonics2023,
  author   = {Wu, Zhenguo and Dai, Liang Yuan and Novick, Asher and Glick, Madeleine and Zhu, Ziyi and Rumley, S{\'e}bastien and Michelogiannakis, George and Shalf, John and Bergman, Keren},
  doi      = {10.1109/JLT.2023.3276588},
  file     = {/Users/wyy/Library/CloudStorage/GoogleDrive-yw3831@columbia.edu/My Drive/Zotero/Wu et al_2023_Peta-Scale Embedded Photonics Architecture for Distributed Deep Learning2.pdf;/Users/wyy/Zotero/storage/V96R3SFX/10124987.html},
  issn     = {0733-8724, 1558-2213},
  journal  = {J. Light. Technol.},
  keywords = {Bandwidth,collective communication,Computational modeling,Copper,Distributed deep learning,optical interconnect,Optical switches,silicon photonics,Silicon photonics,Topology,Training},
  number   = {12},
  pages    = {3737--3749},
  title    = {Peta-{{Scale Embedded Photonics Architecture}} for {{Distributed Deep Learning Applications}}},
  urldate  = {2024-03-18},
  volume   = {41},
  year     = {2023}
}
